{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEemENc_laXK",
        "outputId": "eccaeed3-d66b-4c31-a36e-59aaa6097011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss4.3744, val loss4.3799\n",
            "step 500: training loss1.9200, val loss2.0288\n",
            "step 1000: training loss1.5380, val loss1.7243\n",
            "step 1500: training loss1.3891, val loss1.6158\n",
            "step 2000: training loss1.2988, val loss1.5605\n",
            "step 2500: training loss1.2205, val loss1.5373\n",
            "step 3000: training loss1.1525, val loss1.5437\n",
            "step 3500: training loss1.0768, val loss1.5815\n",
            "step 4000: training loss1.0071, val loss1.6435\n",
            "step 4500: training loss0.8900, val loss1.7504\n",
            "\n",
            "\n",
            "YORK:\n",
            "I being wind, is not to be senator,\n",
            "Or to the desire taste such burnight of her\n",
            "Such thrival, and the head of York in peace\n",
            "Between times, they nothing else tender come\n",
            "They livel but a time like higher base lives,\n",
            "Season'd with childis upon the how year's fall.\n",
            "But then to be gone with tears\n",
            "And in thy drep in sovereignt rich hence;\n",
            "But one post-for those days run with treasons.\n",
            "\n",
            "ROMEO:\n",
            "Is it my true Prince, so it is it and true,\n",
            "Have denied it me state, by trany, tell me,\n",
            "If he may vow,\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters  = 5000\n",
        "eval_intervals = 500\n",
        "learning_rate=3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters= 200\n",
        "n_embed = 384\n",
        "n_heads = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "#head dimension = 384/6 = 64 which is the standard dimension\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt','r',encoding = 'utf-8' ) as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "#mapping character to integer\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } #look up table from character to integer ch:i\n",
        "iots = {i:ch for i,ch in enumerate(chars)} # vice versa from above\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([iots[i] for i in l])\n",
        "\n",
        "#train and test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n= int (0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix= torch.randint(len(data)-block_size, (batch_size,))  # 4 numbers randomly generated between 0 and (len(data)-block_size)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) #stack the 1D data in rows\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad() #telling pytorch that we are not calling backward in this function\n",
        "#this makes efficient use of memory as intermediate values don't have to be stored\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X,Y = get_batch(split)\n",
        "            logits , loss = model(X,Y)\n",
        "            losses[k]=loss.item()\n",
        "        out[split]= losses.mean()\n",
        "    model.train\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    #for single attention\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.key=nn.Linear(n_embed,head_size,bias=False)\n",
        "        self.query=nn.Linear(n_embed,head_size,bias=False)\n",
        "        self.value=nn.Linear(n_embed,head_size,bias=False)\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) #to create tril\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1)*C**-0.5 #to normalize\n",
        "        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "        wei =F.softmax(wei,dim=-1) #B,T,T\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(num_heads*head_size, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        out =  torch.cat([h(x) for h in self.heads],dim =-1) #concat over the channel dimension\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed,4*n_embed), #per token\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embed,n_embed), #this is for projection in feed forward network\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "# we multiplied by 4 in linear part of ffn is based on attention is all you need paper\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    #communication followed by computation\n",
        "    def __init__(self, n_embed, n_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embed//n_heads\n",
        "        self.sa = MultiHeadAttention(n_heads,head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1= nn.LayerNorm(n_embed)\n",
        "        self.ln2= nn.LayerNorm(n_embed)\n",
        "    def forward(self, x):\n",
        "        x= x + self.sa(self.ln1(x))\n",
        "        x= x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "        self.blocks = nn.Sequential(\n",
        "\n",
        "\n",
        "            *[Block(n_embed,n_heads = n_heads) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.sa_head = MultiHeadAttention(4,n_embed//4) # 4 heads of 8 dimensional self attention\n",
        "        self.feedforward = FeedForward(n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    def forward(self,idx,targets=None): #here input is named as idx and target as targets\n",
        "        B,T = idx.shape\n",
        "        token_emb = self.token_embedding_table(idx) #B,T,C where B for batch=4 T for time=8 and C for channel=65\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #T,C\n",
        "        x= token_emb +pos_emb #B,T,C\n",
        "        # x= self.sa_head(x)\n",
        "        # x= self.feedforward(x) #B,T,C\n",
        "        x = self.blocks(x)\n",
        "        x= self.ln_f(x)\n",
        "        logits=self.lm_head(x) #B,T,vocab_size #decoder\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            logits= logits.view(B*T,C)\n",
        "            targets=targets.view(-1) #-1 here means B*T\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits , loss\n",
        "    #to generate from model from B,T to B,T+1  B,T+2 and so on\n",
        "    def generate(self,idx,max_new_tokens):\n",
        "        #idx is (B,T) array of indices\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:,-block_size:] #croping the contect that we feed into self so we don't pass more than box size element\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:,-1,:] #becomes B,C by plucking out last element from time dimension because they are used for future predictions\n",
        "            probs = F.softmax(logits, dim=-1) #B,C\n",
        "            idx_next = torch.multinomial(probs, num_samples= 1) #B,1\n",
        "            idx= torch.cat((idx,idx_next),dim=1) #B,T+1\n",
        "        return idx\n",
        "model= BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "#create optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(),lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_intervals ==0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: training loss{losses['train']:.4f}, val loss{losses['val']:.4f}\")\n",
        "    xb,yb= get_batch('train')\n",
        "    logits , loss = model(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#generate from model\n",
        "context = torch.zeros((1,1),dtype = torch.long, device=device)\n",
        "print(decode(m.generate(context,max_new_tokens=500)[0].tolist())) #0 index to unplug the single batch dimension that exists which gives us time steps i.e 1D array of all indices\n"
      ]
    }
  ]
}
